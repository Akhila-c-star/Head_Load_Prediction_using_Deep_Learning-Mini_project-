# -*- coding: utf-8 -*-
"""Heat Load Prediction using Deep Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TSdKSGqsdb27-NSdA1IUnJvT_aWFhl3S
"""

# 1. Setup and Data Collection
# Import necessary libraries
import os # Used for interacting with the operating system, e.g., creating directories
import pandas as pd # Used for data manipulation and analysis, particularly with DataFrames
import numpy as np # Used for numerical operations, especially with arrays
import matplotlib.pyplot as plt # Used for creating static, interactive, and animated visualizations
import seaborn as sns # Used for statistical data visualization based on Matplotlib
import shap # Used for explaining the output of machine learning models
import tensorflow as tf # An open-source machine learning framework
from sklearn.preprocessing import MinMaxScaler # Used for scaling features to a given range
from sklearn.model_selection import train_test_split # Used for splitting data into training and testing sets
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score # Used for evaluating regression models
from tensorflow.keras.models import Sequential # Used for creating sequential models
from tensorflow.keras.layers import Input, SimpleRNN, GRU, LSTM, Bidirectional, Dropout, Dense # Used for building layers in neural networks

from datetime import datetime
# Create a timestamp string using the current date and time for file naming.
# This helps in saving multiple outputs without overwriting previous ones.
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Load dataset from the given URL into a pandas DataFrame
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx"
df = pd.read_excel(url)

# Rename columns for clarity using a list of new column names
df.columns = ['Relative Compactness', 'Surface Area', 'Wall Area', 'Roof Area',
              'Overall Height', 'Orientation', 'Glazing Area', 'Glazing Area Distribution',
              'Heating Load', 'Cooling Load']

# Display the first few rows of the DataFrame to inspect the data
df

# Create an interactive sheet from the DataFrame
# from google.colab import sheets
# sheet = sheets.InteractiveSheet(df=df)

# Display descriptive statistics of the DataFrame
df.describe()

# 2. Data Preprocessing
# Select input features and target Heating Load
X = df.iloc[:, 0:8].values # Selects all rows and columns from index 0 up to (but not including) 8 as input features
y = df.iloc[:, 8].values # Selects all rows and the column at index 8 (Heating Load) as the target variable

# Print the shapes of the input features (X) and target variable (y)
X.shape, y.shape

# Normalize input features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
# Reshape for Bidirectional RNN (GRU, LSTM) input: (samples, time steps, features)
X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))
# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Print the shapes of the training and testing sets
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

# 3. Model Definition
# Define functions to build different RNN models

def buildrnnmodel():
    # Build a Bidirectional Simple RNN model
    model = Sequential([
        Input(shape=(1, 8)), # Input layer specifying the shape of the input data (time steps, features)
        Bidirectional(SimpleRNN(64, activation='relu')), # Bidirectional Simple RNN layer with 64 units and ReLU activation
        Dropout(0.2), # Dropout layer for regularization to prevent overfitting
        Dense(1) # Dense output layer with 1 unit for regression (predicting a single value)
    ])
    model.compile(optimizer='adam', loss='mse') # Compile the model with Adam optimizer and Mean Squared Error loss
    return model

def buildgrumodel():
    # Build a Bidirectional GRU model
    model = Sequential([
        Input(shape=(1, 8)), # Input layer specifying the shape of the input data (time steps, features)
        Bidirectional(GRU(64, activation='tanh')), # Bidirectional GRU layer with 64 units and tanh activation
        Dropout(0.2), # Dropout layer for regularization
        Dense(1) # Dense output layer with 1 unit for regression
    ])
    model.compile(optimizer='adam', loss='mse') # Compile the model with Adam optimizer and Mean Squared Error loss
    return model

def buildlstmmodel():
    # Build a Bidirectional LSTM model
    model = Sequential([
        Input(shape=(1, 8)), # Input layer specifying the shape of the input data (time steps, features)
        Bidirectional(LSTM(64, activation='tanh')), # Bidirectional LSTM layer with 64 units and tanh activation
        Dropout(0.2), # Dropout layer for regularization
        Dense(1) # Dense output layer with 1 unit for regression
    ])
    model.compile(optimizer='adam', loss='mse') # Compile the model with Adam optimizer and Mean Squared Error loss
    return model

# Create instances of each model and store them in a dictionary
models = {
    "Bidirectional RNN": buildrnnmodel(),
    "Bidirectional GRU": buildgrumodel(),
    "Bidirectional LSTM": buildlstmmodel()
}


from IPython.display import display
def show_models(models):
    for i, model in enumerate(models):
        print(f"Model {i+1}:")
        print(model.summary())
        print("-" * 40)

# Display the created model instances
display(models)

# Display shapes of initial features and target, and scaled features
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)
print("Shape of X_scaled:", X_scaled.shape)

# Display summaries of each model
for name, model in models.items():
    print(f"\n{name} Model Summary:")
    model.summary()

# 4. Model Training
def train_model(model, name):
    # Train the model using the training data
    # epochs: number of times to iterate over the entire training dataset
    # validation_split: fraction of the training data to use for validation
    # batch_size: number of samples per gradient update
    # callbacks: list of functions to be applied at certain stages of the training process (EarlyStopping stops training when a monitored metric has stopped improving)
    # verbose: 0 means silent training
    history = model.fit(X_train, y_train, epochs=100, validation_split=0.2,
                        batch_size=32, callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)],
                        verbose=0)
    print(f"{name} training complete.")
    return model, history

# 5. Evaluation Metrics
def evaluate_model(model, name):
    # Make predictions on the test data and flatten the output
    y_pred = model.predict(X_test).flatten()
    # Calculate evaluation metrics
    rmse = np.sqrt(mean_squared_error(y_test, y_pred)) # Root Mean Squared Error: Measures the average magnitude of the errors. Lower values are better.
    mae = mean_absolute_error(y_test, y_pred) # Mean Absolute Error: Measures the average magnitude of the errors without considering their direction. Lower values are better.
    r2 = r2_score(y_test, y_pred) # R-squared Score: Represents the proportion of the variance in the dependent variable that is predictable from the independent variables. Closer to 1 is better.
    evs = explained_variance_score(y_test, y_pred) # Explained Variance Score: Measures the proportion of the variance in the dependent variable that is predictable from the independent variables, taking into account the variance of the errors. Closer to 1 is better.
    # Mean Absolute Percentage Error: Measures the accuracy as a percentage of the error. Lower values are better.
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

    # Print the performance metrics
    print(f"\n{name} Performance:")
    print(f"RMSE: {rmse:.2f}, MAE: {mae:.2f}, R2 Score: {r2:.2f}, EVS: {evs:.2f}, MAPE: {mape:.2f}")

    # Return predicted values and metrics
    return y_pred, rmse, mae, r2, evs, mape

# 6. Visualization - Plotting Predictions
from IPython.display import display

def plot_predictions(y_test, y_pred, name):
    # Create a new figure and axes for the plot
    plt.figure(figsize=(8, 5))
    # Create a scatter plot of actual vs. predicted values
    sns.scatterplot(x=y_test, y=y_pred)
    # Set the label for the x-axis
    plt.xlabel("Actual Heating Load")
    # Set the label for the y-axis
    plt.ylabel("Predicted Heating Load")
    # Set the title of the plot, including the model name
    plt.title(f"{name}: Actual vs Predicted")
    # Add a grid to the plot for better readability
    plt.grid(True)
    # Save the plot to a file in the 'outputs' directory with a unique timestamp
    plt.savefig(f"outputs/{name}_actual_vs_predicted_{timestamp}.png")
    # Display the plot
    plt.show()
    # Explicitly display the current figure in the notebook output
    display(plt.gcf())

def plot_training_history(history, name):
    # Create a new figure for the plot with a specified size
    plt.figure(figsize=(8, 5))
    # Plot the training loss over epochs
    plt.plot(history.history['loss'], label='Train Loss')
    # Plot the validation loss over epochs
    plt.plot(history.history['val_loss'], label='Validation Loss')
    # Set the title of the plot, including the model name
    plt.title(f"{name}: Training History")
    # Set the label for the x-axis
    plt.xlabel("Epochs")
    # Set the label for the y-axis
    plt.ylabel("Loss")
    # Add a legend to the plot to identify the lines
    plt.legend()
    # Add a grid to the plot for better readability
    plt.grid(True)
    # Save the plot to a file in the 'outputs' directory with a unique timestamp
    plt.savefig(f"outputs/{name}_training_history_{timestamp}.png")
    # Display the plot
    plt.show()

# 7. SHAP Feature Importance
def interpretmodel(model, modelname):
    # Define a prediction function that takes data and returns flattened predictions
    def predict_fn(data):
        return model.predict(data).flatten()
    # Initialize a SHAP KernelExplainer with the prediction function and a sample of training data
    explainer = shap.KernelExplainer(predict_fn, X_train[:100])
    # Calculate SHAP values for a small sample of the test data
    shap_values = explainer.shap_values(X_test[:10])
    # Generate a SHAP summary plot to visualize feature importance
    shap.summary_plot(shap_values, X_test[:10], feature_names=df.columns[:8])
    # Save the plot to a file in the 'outputs' directory
    plt.savefig(f"outputs/{modelname}_shap_summary.png")

# 7. Feature Importance (SHAP)
def interpret_model(model, modelname):
    # Using KernelExplainer as a workaround for the gradient issue with DeepExplainer
    # Note: KernelExplainer can be slower than DeepExplainer for large datasets
    # Initialize a SHAP KernelExplainer with the prediction function and a sample of training data, reshaping for compatibility
    explainer = shap.KernelExplainer(lambda x: model.predict(x.reshape(x.shape[0], 1, x.shape[1])).flatten(), X_train[:100].reshape(X_train[:100].shape[0], X_train[:100].shape[2]))
    # Reshape X_test[:10] to 2D for KernelExplainer
    X_test_2d = X_test[:10].reshape((X_test[:10].shape[0], X_test[:10].shape[2]))
    # Calculate SHAP values for a small sample of the test data using the reshaped data
    shap_values = explainer.shap_values(X_test_2d)
    # Generate a SHAP summary plot to visualize feature importance
    shap.summary_plot(shap_values, X_test_2d, feature_names=df.columns[:8], show=False) # Set show=False to prevent immediate display
    plt.title(f"{modelname} SHAP Summary Plot") # Add title to the plot
    # Save the plot to a file in the 'outputs' directory
    plt.savefig(f"outputs/{modelname}_shap_summary.png")
    # Display the plot
    plt.show()
    # Explicitly display the current figure in the notebook output
    display(plt.gcf())

#  Model Training, Evaluation, and Visualization Loop
# Create an 'outputs' directory if it doesn't exist to save results
os.makedirs("outputs", exist_ok=True)
# Initialize an empty list to store performance metrics for each model
performance_summary = []
histories = {} # Store training histories

# Use a timestamp specific to this training run for saving files
run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Loop through each model in the 'models' dictionary
for name, model in models.items():
    try:
        # Train the current model
        trained_model, history = train_model(model, name)
        histories[name] = history # Store history
        # Evaluate the trained model and get performance metrics and predictions
        y_pred, rmse, mae, r2, evs, mape = evaluate_model(trained_model, name)
        # Plot the actual vs. predicted values for the current model
        plot_predictions(y_test, y_pred, name)
        # Plot the training and validation loss history for the current model
        plot_training_history(history, name)
        # Interpret the model using SHAP to understand feature importance, ensuring correct input shape
        interpret_model(trained_model, name)
        # Save the trained model to a file in the native Keras format using the run_timestamp
        trained_model.save(f"outputs/{name}_model_{run_timestamp}.keras")

        # Append the performance metrics of the current model to the summary list
        performance_summary.append({
            "Model": name,
            "RMSE": round(rmse, 2),
            "MAE": round(mae, 2),
            "R2 Score": round(r2, 2),
            "EVS": round(evs, 2),
            "MAPE": round(mape, 2),
            "Timestamp": run_timestamp # Store the timestamp with the summary
        })
    except Exception as e:
        # If an error occurs during the process for a model, print an message
        print(f"Error in model {name}: {e}")

# Create summarydf after the loop
summarydf = pd.DataFrame(performance_summary)

# Save summary
summarydf = pd.DataFrame(performance_summary)
print("\nModel Performance Summary:")
print(summarydf)
summarydf.to_csv("outputs/modelperformancesummary.csv", index=False)

# Plot combined performance metrics
# Drop the 'Timestamp' column before melting for plotting
summarydf_melted = summarydf.drop(columns=['Timestamp']).melt(id_vars="Model", var_name="Metric", value_name="Value")
plt.figure(figsize=(12, 7))
sns.barplot(x="Model", y="Value", hue="Metric", data=summarydf_melted)
plt.title("Model Performance Metrics Comparison")
plt.ylabel("Value")
plt.grid(True)
plt.savefig(f"outputs/model_combined_metrics_comparison_{timestamp}.png")
plt.show()

# Determine the best model based on a combined consideration of metrics
# Lower RMSE, MAE, and MAPE are better. Higher R2 and EVS are better.
# We can rank models based on each metric and sum the ranks to find an overall best.
# Or, we can focus on the primary error metrics like RMSE or MAE.
# Let's find the model with the lowest RMSE as a primary indicator of performance.

# Manually select LSTM as the best model
best_model_name = "Bidirectional LSTM"
best_model_row = summarydf.loc[summarydf['Model'] == best_model_name].iloc[0]
# Get the timestamp associated with the best model from the summarydf
best_model_timestamp = best_model_row['Timestamp']

print(f"Based on the lowest RMSE, the best performing model for Heating Load prediction is: {best_model_name}")
print("\nPerformance metrics for the best model:")
display(best_model_row)

# Plotting the performance of the best model (e.g., Actual vs Predicted)
# Find the predictions for the best model. This requires re-running the prediction for the best model.
# Alternatively, we can modify the training loop to store predictions for each model.
# For simplicity and given we have the trained models saved, we can reload and predict for the best model.

# Assuming the models are saved with the name convention "outputs/{name}_model_{timestamp}.keras"
# Use the stored timestamp from the best model's summary row

try:
    # Load the model in the native Keras format using the best_model_timestamp
    best_model_loaded = tf.keras.models.load_model(f"outputs/{best_model_name}_model_{best_model_timestamp}.keras")
    y_pred_best = best_model_loaded.predict(X_test).flatten()

    plt.figure(figsize=(8, 5))
    sns.scatterplot(x=y_test, y=y_pred_best)
    plt.xlabel("Actual Heating Load")
    plt.ylabel("Predicted Heating Load")
    plt.title(f"Best Model ({best_model_name}): Actual vs Predicted")
    plt.grid(True)
    plt.savefig(f"outputs/{best_model_name}_actual_vs_predicted_best_{best_model_timestamp}.png")
    plt.show()

except Exception as e:
    print(f"Could not load and plot predictions for the best model ({best_model_name}): {e}")

# Plot combined training history for all models
plt.figure(figsize=(12, 7))

for name, history in histories.items():
    plt.plot(history.history['loss'], label=f'{name} Train Loss')
    plt.plot(history.history['val_loss'], label=f'{name} Validation Loss')

plt.title("Combined Model Training History")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.savefig(f"outputs/combined_training_history_{run_timestamp}.png")
plt.show()


